{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a069e75-0d04-4dc8-b121-6de1d657646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required modules. \n",
    "import random \n",
    "import json \n",
    "import pickle \n",
    "import numpy as np \n",
    "import nltk \n",
    "  \n",
    "from keras.models import Sequential \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from keras.layers import Dense, Activation, Dropout \n",
    "from keras.optimizers import SGD \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e09fb-045c-49d2-9b1c-96b5f085f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the json.intense file \n",
    "intents = json.loads(open(\"intents/intents.json\").read()) \n",
    "  \n",
    "# creating empty lists to store data \n",
    "words = [] \n",
    "classes = [] \n",
    "documents = [] \n",
    "ignore_letters = [\"?\", \"!\", \".\", \",\"] \n",
    "for intent in intents['intents']: \n",
    "    for pattern in intent['patterns']: \n",
    "        # separating words from patterns \n",
    "        word_list = nltk.word_tokenize(pattern) \n",
    "        words.extend(word_list)  # and adding them to words list \n",
    "          \n",
    "        # associating patterns with respective tags \n",
    "        documents.append(((word_list), intent['tag'])) \n",
    "  \n",
    "        # appending the tags to the class list \n",
    "        if intent['tag'] not in classes: \n",
    "            classes.append(intent['tag']) \n",
    "  \n",
    "# storing the root words or lemma \n",
    "words = [lemmatizer.lemmatize(word) \n",
    "         for word in words if word not in ignore_letters] \n",
    "words = sorted(set(words)) \n",
    "  \n",
    "# saving the words and classes list to binary files \n",
    "pickle.dump(words, open('words.pkl', 'wb')) \n",
    "pickle.dump(classes, open('classes.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8d2b0-7ec2-42a3-ab17-1c2069ee4a6a",
   "metadata": {},
   "source": [
    "This cell create the training data for the model training as train_x as input and train_y as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c56714-e43d-40eb-a80e-75af185b504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need numerical values of the \n",
    "# words because a neural network \n",
    "# needs numerical values to work with \n",
    "training = [] \n",
    "output_empty = [0]*len(classes) \n",
    "for document in documents: \n",
    "    bag = [] \n",
    "    word_patterns = document[0] \n",
    "    word_patterns = [lemmatizer.lemmatize( \n",
    "        word.lower()) for word in word_patterns] \n",
    "    for word in words: \n",
    "        bag.append(1) if word in word_patterns else bag.append(0) \n",
    "          \n",
    "    # making a copy of the output_empty \n",
    "    output_row = list(output_empty) \n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append((np.array(bag), np.array(output_row))) \n",
    "random.shuffle(training) \n",
    "#training = np.array(training) \n",
    "  \n",
    "# splitting the data\n",
    "train_x = np.array([item[0] for item in training])\n",
    "train_y = np.array([item[1] for item in training])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddae27f-3279-479e-8122-603967a778da",
   "metadata": {},
   "source": [
    "This cell builds and train the model for the chatbot which is chatbotmodel.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f116bf-1501-4af3-a221-2a9b32eee604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Sequential machine learning model \n",
    "model = Sequential() \n",
    "model.add(Dense(128, input_shape=(len(train_x[0]), ), \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(len(train_y[0]),  \n",
    "                activation='softmax'))\n",
    "\n",
    "# compiling the model \n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True) \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, metrics=['accuracy']) \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), \n",
    "                 epochs=200, batch_size=5, verbose=1) \n",
    "\n",
    "# saving the model \n",
    "model.save(\"chatbotmodel.h5\", hist) \n",
    "  \n",
    "# print statement to show the  \n",
    "# successful training of the Chatbot model \n",
    "print(\"Yay!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
